# Urdu-BERT Pretraining (PyTorch)

I implementated BERT pre training for **Urdu language**, using **Masked Language Modeling (MLM)** as the primary objective****
---

## Implementation Highlight

- Implements **dynamic masking** per epoch during training, instead of static masking.
- Enhances token diversity and generalization by regenerating masked tokens each time a sequence is reused.

---
